{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**TSWA PRACTICAL 7 - Text Normalization**"
      ],
      "metadata": {
        "id": "9id-K3EbLL8J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uHK-TNkLH0-",
        "outputId": "922a42b1-7046-4a72-a81a-f445034e6b0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk"
      ],
      "metadata": {
        "id": "HDkuTPzlLSLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize , sent_tokenize\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "VAqDrbVuLVny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41RaKAf5LYYg",
        "outputId": "5dd212ff-fdff-4589-b023-5cc41e55db0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions"
      ],
      "metadata": {
        "id": "dUpWgeC_Mei3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def contraction_remover(text):\n",
        "    expanded_words = []\n",
        "    for word in text.split():\n",
        "    # using contractions.fix to expand the shortened words\n",
        "        expanded_words.append(contractions.fix(word))\n",
        "    expanded_text = ' '.join(expanded_words)"
      ],
      "metadata": {
        "id": "oFaz7r_DLcR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_corpus(df):\n",
        "    # Remove HTML tags\n",
        "    df['Preprocess_Article'] = df['Article'].apply(lambda x: re.sub(r'<.*?>', '', x))\n",
        "\n",
        "    # Convert to lowercase\n",
        "    df['Preprocess_Article'] = df['Preprocess_Article'].str.lower()\n",
        "\n",
        "    # Remove URLs\n",
        "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'http\\S+|www\\.\\S+', '', x))\n",
        "\n",
        "    # Remove email addresses\n",
        "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\S+@\\S+', '', x))\n",
        "\n",
        "    # Remove phone numbers\n",
        "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\d{10}', '', x))\n",
        "\n",
        "    # Handle negation\n",
        "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\bnot\\b(\\w+)', r'not_\\1', x))\n",
        "\n",
        "    # Remove special characters and punctuation\n",
        "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\\s]', '', x))\n",
        "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
        "\n",
        "    # Remove numeric tokens\n",
        "    df['Preprocess_Article'] = df['Preprocess_Article'].apply(lambda x: re.sub(r'\\b\\d+\\b', '', x))\n",
        "\n",
        "    # Tokenization\n",
        "    df['tokens'] = df['Preprocess_Article'].apply(word_tokenize)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    df['tokens'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    df['tokens'] = df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    df['tokens'] = df['tokens'].apply(lambda x: [lemmatizer.lemmatize(word,get_wordnet_pos(word))for word in x])\n",
        "\n",
        "     # Convert tokens into a single string\n",
        "    df['Clean Article'] = df['tokens'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "qzqFBP8uLzFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to map POS tag to WordNet POS tag\n",
        "def get_wordnet_pos(word):\n",
        "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "  tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "  return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "mKrwUzneL2cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "metadata": {
        "id": "F3SpK4yyL8Dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch data\n",
        "\n",
        "data = fetch_20newsgroups(subset='all')\n",
        "data = fetch_20newsgroups(subset='all', shuffle=True,remove=('headers', 'footers', 'quotes'))\n",
        "data_labels_map = dict(enumerate(data.target_names))"
      ],
      "metadata": {
        "id": "VI3VKRgbMjiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create objects for each package\n",
        "import numpy as np\n",
        "#import text_normalizer as tn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "UZAOwYSXL_cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building the dataframe for the data extracted from newgroups\n",
        "# Create a corpus of newsgroup sentences and create the data frame\n",
        "corpus=data.data\n",
        "target_labels=data.target\n",
        "target_names = [data_labels_map[label] for label in data.target]\n",
        "data_df = pd.DataFrame({'Article': corpus, 'Target Label': target_labels,'Target Name': target_names})\n",
        "print(data_df.shape)\n",
        "data_df.head(10)"
      ],
      "metadata": {
        "id": "EcCaOMghMOOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "8ATpfxgQMOMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize_corpus(data_df)"
      ],
      "metadata": {
        "id": "m3Y_rnDgMOJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.info()"
      ],
      "metadata": {
        "id": "q9sEnrWwMOGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# view sample data\n",
        "data_df = data_df[['Article', 'Clean Article', 'Target Label', 'Target Name']]\n",
        "data_df.head(10)"
      ],
      "metadata": {
        "id": "oEhai_QEMN11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any unwanted characters\n",
        "data_df = data_df.replace(r'^(\\s?)+$', np.nan, regex=True)\n",
        "data_df.info()"
      ],
      "metadata": {
        "id": "ngdQlkuMM3Kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = data_df.dropna().reset_index(drop=True)\n",
        "data_df.info()"
      ],
      "metadata": {
        "id": "EhL32lxpM45t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a csv file of the cleaned doucment so that it can be reused\n",
        "data_df.to_csv('clean_newsgroups.csv', index=False)"
      ],
      "metadata": {
        "id": "xN2IOdLUM6Z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data - training and testing\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "TdoKmN0pM6V-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names = train_test_split(np.array(data_df['Clean Article']), np.array(data_df['Target Label']),\n",
        "np.array(data_df['Target Name']),test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "8f41r6EXM-BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dictionary for train and test data\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "PKpvyRJKNAwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trd = dict(Counter(train_label_names))\n",
        "tsd = dict(Counter(test_label_names))"
      ],
      "metadata": {
        "id": "KLHa8a4wNCuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trd"
      ],
      "metadata": {
        "id": "_DHkBkPkNGTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tsd"
      ],
      "metadata": {
        "id": "uLmfrJFGNIGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(pd.DataFrame([[key, trd[key], tsd[key]] for key in trd],columns=['Target Label', 'Train Count', 'Test Count'])\n",
        ".sort_values(by=['Train Count', 'Test Count'],ascending=False))"
      ],
      "metadata": {
        "id": "L3XjV9hTNIC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iKReBXNLNTiG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}